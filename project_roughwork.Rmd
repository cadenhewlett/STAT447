---
title: "Project Work Example"
author: "Caden Hewlett"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{bbm}
   - \usepackage{upgreek}
   - \usepackage{booktabs}
output: pdf_document
---

```{r setup, include=FALSE}
library(extraDistr)
library(ggplot2)
library(pracma)
library(distr)
library(latex2exp)
require(reticulate, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(rstan, quietly = TRUE)
require(knitr, quietly = TRUE)
```

The *Bellman Equation* for the frequentist Q-learning update model is given by. 

$$
\begin{aligned}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \Big[r_{t}  + \gamma 
\, \underset{a \in A}{\text{argmax}}\big\{Q(s_{t+1}, a) \big\} -  Q(s_t, a_t) \Big]
\end{aligned}
$$
In the literature, a 4-tuple Markov Decision Process (MDP) $\langle {S}, {A}, p_t, p_r\rangle$ is considered; where ${S}$ is the state set (assumed to be discrete) and $A$ is the action set. Critically, 4-tuple model includes $p_t$,  the probability that action $a_t$ at time $t$ actually sends you to the target $s_{t+1}$, where $(s_t \overset{a_t \in A}{\longrightarrow} s_{t+1}).$ This concept can be likened to the real-world example of training to investment given a portfolio; there is some probability $p_t$ that investing in a given company ($a \in A$) will actually result in an increased market share ($s_{t+1} \in S$.) In a simplified path finding model, however, we assume that the transition probabilities given action $a$ at time $t$ is not stochastic. In other words, 
$$ \forall t\in \mathbb{Z}^{+},  \forall (s_i, s_j) \in S,\, \forall a \in A \text{ s.t. } (s_i\overset{a}{\rightarrow} s_{j}),   \; \mathbb{P}(S_{t+1} = s_{t+1}\mid \{a_t, s_t\}) \equiv \underbrace{p_t(s \overset{a}{\rightarrow} t)}_{\text{from literature}} =1$$
Further, in the four-tuple model there is $p_r$; similar to $p_t$, it is the probability that we receive reward $r$ in the rewards set $R$ given we arrive at state $s_{t+1}$ after taking  $(s_t \overset{a_t \in A}{\longrightarrow} s_{t+1})$ at time $t$. In the investment example, this means that given the investment and increased market share, $p_r$ is the probability of a given return on investment $r \in R \subseteq \mathbb{R}$. In the pathfinding example, there isn't need for $p_r$ in the initial implementation (though this may be revisited eventually as Bayesian models may perform better on $p_r$.) We assume that the rewards set $R$ is defined by a deterministic function $h(\cdot \mid s)$ or, more simply, that $R$ is the \underline{realization} of a random variable, but is itself non-random.


In effect, the preliminary implementation is a 2-tuple MDP $\langle {S}, {A} \rangle$, which is more in-line with "[simplified](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)" $Q$-Learning implementations 



in a Bayesian sense

$$
\begin{aligned}
V^{\star}(s) = \underset{a \in A}{\text{argmax}}\big\{Q(s_{t+1}, a) \big\}
\end{aligned}
$$


NG distributions are self-conjugates, 


Rather than maintain $s \times a$ rewards distributions, the authors use Assumption 3 to maintain a tuple of hyper-parameters $\rho_{s,a} = \langle \mu_0^{s,a}, \lambda_0^{s,a}, \alpha_0^{s,a}, \beta_0^{s,a} \rangle$. In other words, for data optimization, the collections of hyperparameters are used for the state-action pairs rather than distributions or, critically, $Q$-values. 


**Action Selection**

Q-Sampling:

$$
\mathbb{P}(a = \underset{a' \in \,\mathcal{A}}{\text{argmax}}) = \mathbb{P}(\forall a', a \in \mathcal{A} \text{ s.t. } a' \neq a,(\mu_{s, a} > \mu_{s, a'}))
$$
Probability of taking action $a$ based on the current learned belief that it is optimal. This is then found by marginalizing the normal-gamma with respect to the observed $Q$-table values indexing at $a$ for the "slicing" of $Q(s,a)$. 


The literature shows that if $p(\mu, \tai) \sim \text{NG}(\mu_0, \lambda, \alpha, \beta)$ that the CDF of the marginalized $p(\mu)$ is

$$
\mathbb{P}(\mu < k) = F(k; \mu) = F_{\text{T}}(\varkappa ;  \; 2\alpha), \text{ where } \varkappa = (k - \mu_0)\sqrt{\frac{\lambda a}{\beta}}
$$

$$
p_{\mu} \sim \text{T}\bigg((k - \mu_0)\sqrt{\frac{\lambda a}{\beta}}, \; 2\alpha \bigg)
$$
Which has expectation $\mu_0$ and variance $\beta / \lambda (\alpha - 1)$. 

This is used to sample each candidate state-action pair from the agent's location $s_{t}$. The highest sampled value is then preferred.  



**Moment Updating**


Randomly sample $R_t^{1}, \dots R_t^n$ from the distribution.


Update $\mathbb{P}(R_{s, a})$ with the sampled $\sum_{i \in [1,n]}(r + \gamma R_{t})$, where each sample has weight $1/n$. 

<!-- \langle \mu_0^{s,a}, \lambda_0^{s,a}, \alpha_0^{s,a}, \beta_0^{s,a} \rangle -->



Let $s \rightarrow {t}$  by action $a$ yielding reward $r$ with an overall learning rate $\gamma$.


The moment updating equations were configured by combining *Lemma 3.4* with the sample moments and conjugate updating Theorem 3.1 presented in the work.


Specifically, if $R_t \sim N(\mu, \tau)$ where mean $\mu$ and precision $\tau$ are characterized by a $\text{NG}(\mu_0, \lambda, \alpha, \beta)$ distribution, then $\mathbb{E}(R_t) = \mu_0$ and $\mathbb{E}(R^2) = \Big( \dfrac{\lambda + 1}{\lambda} \cdot \dfrac{\beta}{\alpha-1} + \mu_0^2\Big)$.


Then, if we have sampled $R_t^{(1)}, \dots R_t^{(n)}$, the limit as $n \rightarrow \infty$ of the first two moments of the set of modified samples $\big\{r + \gamma R_t^{(i)} \big\}_{i =1}^n$ is given by"
$$
\begin{aligned}
\mathbb{M}_1 &= r + \gamma \mathbb{E}(R_t) \\
\mathbb{M}_2 &= r^2 + 2 \gamma r \mathbb{E}(R_t) + \gamma^2 \mathbb{E}(R_t^2) 
\end{aligned}
$$
Combining these results leads to the implemented moment updating step:
$$
\begin{aligned}
\mathbb{M}_1 &= r + \gamma \mathbb{E}(R_t) = r + \gamma\mu_0 \\
\mathbb{M}_2 &= r^2 + 2 \gamma r \mathbb{E}(R_t) + \gamma^2 \mathbb{E}(R_t^2) = r^2 + 2 \gamma r \mu_0 + \gamma^2\Big( \frac{\lambda + 1}{\lambda} \cdot \dfrac{\beta}{\alpha-1} + \mu_0^2\Big)
\end{aligned}
$$



$$
\begin{aligned}
p_{t-1}(\mu, \tau) &\sim \text{NG}(\mu_0, \lambda, \alpha, \beta) \\
R_{i}  \mid \mu, \tau &\sim N(\mu, \tau)
\\
\mu^{t}_0 &= \dfrac{\lambda\mu_0 + n \mathbb{M}_1}{\lambda + n} \\
\lambda^{t} &= \lambda + n \\
\alpha^{t} &= \alpha^{s,a} + \dfrac{1}{2}n \\
\beta^{t} &= \beta +\dfrac{1}{2}n (\mathbb{M}_2 - \mathbb{M}_1^2) + \dfrac{n \lambda(\mathbb{M_1 - \mu_0})^2}{2(\lambda + n)} \\
p_t(\mu, \tau \mid \{r_i\}_{i =1}^n) &\sim \text{NG}(\mu_0^t, \lambda^t, \alpha^t, \beta^t)
\end{aligned}
$$
<!-- Example of using Python and `reticulate`, as well as `py2tex` to convert the output to natural latex. -->
<!-- ```{python, results = FALSE} -->
<!-- from sympy import symbols, simplify -->
<!-- from pytexit import py2tex -->

<!-- # Define symbols -->
<!-- a = symbols('a') -->

<!-- # Original equation's rearrangement -->
<!-- left_side_coefficient = 1 - (1/(6*a)) -->
<!-- right_side = (3 - 2*a) / (6*a) -->

<!-- # Simplify the equation for p(1) -->
<!-- rho_1 = simplify(right_side / left_side_coefficient) -->
<!-- # sympy.latex(eval(rho_1)) -->
<!-- pytex_obj = py2tex(str(rho_1)) -->
<!-- ``` -->
<!-- Then, in the `R` cell, by setting `results = 'asis'`, we render the equation directly.  -->
<!-- ```{r, results='asis'} -->
<!-- cat(py$pytex_obj) -->
<!-- ``` -->
