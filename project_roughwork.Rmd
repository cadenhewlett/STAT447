---
title: "Project Work Example"
author: "Caden Hewlett"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{bbm}
   - \usepackage{upgreek}
   - \usepackage{booktabs}
output: pdf_document
---

```{r setup, include=FALSE}
library(extraDistr)
library(ggplot2)
library(pracma)
library(distr)
library(latex2exp)
require(reticulate, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(rstan, quietly = TRUE)
require(knitr, quietly = TRUE)
```

The *Bellman Equation* for the frequentist Q-learning update model is given by. 

$$
\begin{aligned}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \Big[r_{t}  + \gamma 
\, \underset{a \in A}{\text{argmax}}\big\{Q(s_{t+1}, a) \big\} -  Q(s_t, a_t) \Big]
\end{aligned}
$$
In the literature, a 4-tuple Markov Decision Process (MDP) $\langle {S}, {A}, p_t, p_r\rangle$ is considered; where ${S}$ is the state set (assumed to be discrete) and $A$ is the action set. Critically, 4-tuple model includes $p_t$,  the probability that action $a_t$ at time $t$ actually sends you to the target $s_{t+1}$, where $(s_t \overset{a_t \in A}{\longrightarrow} s_{t+1}).$ This concept can be likened to the real-world example of training to investment given a portfolio; there is some probability $p_t$ that investing in a given company ($a \in A$) will actually result in an increased market share ($s_{t+1} \in S$.) In a simplified pathfinding model, however, we assume that the transition probabilities given action $a$ at time $t$ is not stochastic. In other words, 
$$ \forall t\in \mathbb{Z}^{+},  \forall (s_i, s_j) \in S,\, \forall a \in A \text{ s.t. } (s_i\overset{a}{\rightarrow} s_{j}),   \; \mathbb{P}(S_{t+1} = s_{t+1}\mid \{a_t, s_t\}) \equiv \underbrace{p_t(s \overset{a}{\rightarrow} t)}_{\text{from literature}} =1$$
Further, in the four-tuple model there is $p_r$; similar to $p_t$, it is the probability that we receive reward $r$ in the rewards set $R$ given we arrive at state $s_{t+1}$ after taking  $(s_t \overset{a_t \in A}{\longrightarrow} s_{t+1})$ at time $t$. In the investment example, this means that given the investment and increased market share, $p_r$ is the probability of a given return on investment $r \in R \subseteq \mathbb{R}$. In the pathfinding example, there isn't need for $p_r$ in the initial implementation (though this may be revisited eventually as Bayesian models may perform better on $p_r$.) We assume that the rewards set $R$ is defined by a deterministic function $h(\cdot \mid s)$ or, more simply, that $R$ is the \underline{realization} of a random variable, but is itself non-random.


In effect, the preliminary implementation is a 2-tuple MDP $\langle {S}, {A} \rangle$, which is more in-line with "[simplified](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292)" $Q$-Learning implementations 



in a Bayesian sense

$$
\begin{aligned}
V^{\star}(s) = \underset{a \in A}{\text{argmax}}\big\{Q(s_{t+1}, a) \big\}
\end{aligned}
$$

<!-- Example of using Python and `reticulate`, as well as `py2tex` to convert the output to natural latex. -->
<!-- ```{python, results = FALSE} -->
<!-- from sympy import symbols, simplify -->
<!-- from pytexit import py2tex -->

<!-- # Define symbols -->
<!-- a = symbols('a') -->

<!-- # Original equation's rearrangement -->
<!-- left_side_coefficient = 1 - (1/(6*a)) -->
<!-- right_side = (3 - 2*a) / (6*a) -->

<!-- # Simplify the equation for p(1) -->
<!-- rho_1 = simplify(right_side / left_side_coefficient) -->
<!-- # sympy.latex(eval(rho_1)) -->
<!-- pytex_obj = py2tex(str(rho_1)) -->
<!-- ``` -->
<!-- Then, in the `R` cell, by setting `results = 'asis'`, we render the equation directly.  -->
<!-- ```{r, results='asis'} -->
<!-- cat(py$pytex_obj) -->
<!-- ``` -->
