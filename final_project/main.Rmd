---
title: "Leveraging Conjugacy in Dirichlet Process Poisson Mixture Models"
subtitle: "STAT 447 Final Project"
author: "Caden Hewlett"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
   - \usepackage{bbm}
   - \usepackage{upgreek}
   - \usepackage{booktabs}
   - \usepackage{bbm}
   - \usepackage{mathrsfs}
   - \usepackage{yfonts}
   - \usepackage{wrapfig}

bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# https://stats.stackexchange.com/questions/95120/simulate-dirichlet-process-in-r
```

## Introduction

```{r}
# TODO
```

We will begin with a brief literature review, discussing probability measures and Dirichlet Processes from a theoretical standpoint. In the Data Analysis section, we briefly recap the source data aggregation and imputation, inspect the plot, and provide some summary statistics. We will proceed to detail the stick-breaking process used by the algorithm and formally define both the Bayesian model and hyper-parameter choices for the DPMM. In the Results section, we produce the *a posteriori* results, including the table of weighted posterior rates and the posterior distribution plot. We then briefly interpret these results in the context of the problem domain and discuss shortcomings and future work.


\vspace{-5mm}


## Literature Review 

\vspace{-4mm}

Before we discuss Dirichlet Processes, it is crucial to establish a groundwork in probability measure theory. We will briefly revisit the concepts of $\sigma$-algebra and probability measures. In the following, we present generalized definitions which are discussed in rigor works such as [@billingsley2012probability] and [@rudin1986real].

\vspace{-5mm}

### Measure Theory

\vspace{-3mm}

Let $\mathbb{X}$ be a well-defined sample space. A $\sigma$-algebra $\mathcal{F} \subseteq P(\mathbb{X})$ is a set satisfying the following:

\vspace{-3mm}

  1. The entire sample space $\mathbb{X}$ is in $\mathcal{F}$ and $\emptyset$ is in $\mathcal{F}$. This is referred to in the literature as "non-emptiness and universality." 
  2. For all sets $A \in \mathcal{F}$, the complement $A^c \in \mathcal{F}$. This property is referred to as "closure under complementation."
  3. For any countable collection of sets $\{A_i\}_{i \in I}$, where $I$ is a countable index set, if $\forall i \in I, A_i \in \mathcal{F}$ then $\bigcup_{i \in I}A_i \in \mathcal{F}.$ This is referred to as "closure under countable unions."


\vspace{-1mm}


For the sake of this work, we are more interested in *probability measures*, which are built on $\sigma$-algebra. A probability measure $\upmu : \mathcal{F} \mapsto [0,1]$ satisfies the following familiar axioms of probability:


\vspace{-4mm}

  1. $\forall A \in \mathcal{F}, \;  \upmu(A) \geq 0$. This is referred to as "non-negativity."
  2. $\upmu(\mathbb{X}) = 1$, and $\upmu(\emptyset) = 0$. 
  3. For any countable set $\{A_i\} \subseteq \mathcal{F}$ where $\forall i \neq j, A_i \cap A_j = \emptyset$, we have that $\upmu\Big(\bigcup_{i=1}^{\infty} A_i \Big)  = \sum_{i =1}^{\infty} \upmu(A_i)$, this is referred to as "countable additivity."


Before we move on to Dirichlet Processes, we acknowledge that the above definitions might be abstract for those new to measure theory. For clarity, the Appendix includes examples to illustrate $\sigma$-algebra properties and verify a probability measure $\upmu$ on a simple finite set.


### Dirichlett Process: A Distribution Over Measures

\vspace{-1.5mm}

Now, we introduce the concept of the Dirichlet Process (DP), a distribution across probability measures. We adopt the description used in [@hannah2011dirichlet], where the DP is specified by its base measure $\mathbb{G}_0$ (commonly a distribution) and the concentration parameter, $\alpha$. A random sample from a Dirichlet Process is a probability measure over a $\sigma$-algebra from sample space $\mathbb{X}$, which is often assumed to be countable. The resulting probability measure follows the structure of the base measure $\mathbb{G}_0$ with a degree of deviation controlled by $\alpha$. A Dirichlet Process can also be thought of conceptually of as a Dirichlet Distribution whose support is an infinite-simplex, rather than a discretely-defined $k$-simplex. [@Ferguson1973]


The $\alpha$ parameter indicates less confidence in the base measure, yielding greater dispersion and a higher number of clusters of measures about $\mathbb{G}_0$. Conversely, a lower $\alpha$ indicates more confidence in $\mathbb{G}_0$ resulting in fewer, larger clusters. The finite approximation of this property is explored in more detail in the Methods section.  



## Data Analysis and Processing


Now, we will perform some exploratory data analysis and explain the data set used. The data is sourced from the Chicago Police Department [@chicago_police_department__cpd__2024] and contains $794,956$ vehicle accident reports from 2014 to 2023. A hexplot of the aggregated results is below.
\begin{wrapfigure}{r}{0.64\textwidth}
\vspace{-9mm}
\begin{center}
\includegraphics[width=0.64\textwidth]{data_raw.PNG}
\end{center}
\vspace{-6mm}
\end{wrapfigure}
We applied an aggregation code framework modified from [@stackoverflow2023customfunc] in order to coerce the data into weekly crash counts over the time frame. Certain weeks in both $2014$ and $2015$ had missing counts. In these cases, we performed nearest-neighbours imputation for the missing values. In addition, we grouped the counts by severity to craft an environment in which reports were logged by monetary crash damage, with low-severity accidents first in order.  Furthermore, in an attempt to avoid overflow in training loops we measured crashes in hundreds. As the hexplot shows, there are three distinct categorizations of crash counts, with the count dispersion decreasing across the groups. In addition, as the plot shows, the point density varies across these clusters. These characteristics of the aggregated data create a distinct multimodal plot, which makes these data a good candidate for Bayesian non-parametrics. The code for data processing and plotting are included in the appendix. 


\newpage

<!-- Using  -->
<!-- ```{r} -->
<!-- # TODO: discuss data itself -->
<!-- #       discuss aggregation and latent categorization (and events where this could take place irl) -->
<!-- #       discuss 2014 imputation process (by nearest neighbours) -->
<!-- # What we effectively have is discrete weekly counts, with latent categorization (damage severity) that is masked to the reporter. however, with a DP Infinite Mixture Model we can still recover a posteriori estimates of the Poisson rate parameters of each of the latent categories (despite the fact that the count distribution is multimodal) and present a unified posterior model -->
<!-- ``` -->


## Methods


We will use a finite-approximation of a Dirichlet Process centered about a gamma base measure using the stick-breaking methodology. In addition, we will utilize the gamma-poisson conjugacy to implicitly define the mixing distribution, which will be utilized be used as the `MixingDistribution` object in the MCMC DP sampler implemented in the `dirichletprocess` package [@dirichletprocess2023] to produce a posterior distribution. 


In this section, we outline the stick-breaking process used by the algorithm and formally define both the Bayesian model and hyper-parameter choices for the DPMM.

[@Sethuraman1994] and [@teh2006dirichlet]
```{r}
# TODO: First, Discuss finite-approximation via GEM
# TODO: Then discuss implementation via custom Mixing Distribution : https://en.wikipedia.org/wiki/Mixture_distribution
# https://dm13450.github.io/2018/02/21/Custom-Distributions-Conjugate.html
# TODO: noting that when deciding on a prior-likelihood model, the characterization
# of the system for a well-functioning mixture distribution (to be used in the DPMM)
# requires the following four characteristics, which we derive from the gamma-poisson conjugacy
# it's important to recall that while this conjugate system allows us to well-define the parameters
# for the mixing distribution as required for a Dirichlett Process sampler, the gamma distribution 
# in the prior-likelihood pair is the base measure, not the prior. we can think of these as the 
# parameters to the mixing process. (i.e. requires a Mixing Distribution Object `mdObject` which is 
# implicitly defined by the following four features: (4 functions))

# TODO: when setup is done, declare model equations
```

## Results 

```{r}
# Total Runtime of Script:   691.36 seconds
# TODO: generation from the Gibbs sampling procedure detailed in [@Neal2000]
# TODO: report posterior distrubtion (of mean of lambda with CrI error bars) from posterior frame
# TODO: report posterior weights and lambda values (much more useful, actually shows high
# weight on three separate lambdas )
# the original source data has three distinct clusters which is why the non-paramteric mixture model 
# worked better, and the varying rates between those clusters is well captured by 
# the plot of the posterior which is encouraging ability to adapt to the structure of the data
```

<!-- What we effectively have is discrete weekly counts, with latent categorization (damage severity) that is masked to the reporter. however, with a DP Infinite Mixture Model we can still recover a posteriori estimates of the Poisson rate parameters of each of the latent categories (despite the fact that the count distribution is multimodal) and present a unified posterior model -->


```{r}
# TODO: Fit to Chicago Traffic model. Discuss posterior distribution curve.

# we've actually ended up creating latent categorization by 
# crash severity, which is exactly what we are looking for 
# we can think of this as "evaluated weekly crash reports"
# assuming that more severe crashes will be processed later
# or, more likely, the number of evaluated crash reports 
# declines in groups over time as severity increases
# since more severe reports take longer to evaluate
```




## Conclusion and Further Work

```{r}
# TODO: Discuss Chicago Traffic Model and Implementation
```

<!--  https://mlg.eng.cam.ac.uk/zoubin/tut06/ywt.pdf -->
<!-- https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/A-Bayesian-Analysis-of-Some-Nonparametric-Problems/10.1214/aos/1176342360.full -->
<!-- https://en.wikipedia.org/wiki/Dirichlet_process -->
<!-- https://en.wikipedia.org/wiki/Dirichlet_process#cite_note-2 -->
<!-- https://en.wikipedia.org/wiki/Probability_measure -->
<!-- https://en.wikipedia.org/wiki/Sigma-additive_set_function -->


# Appendix 


*Acknowledgements*: Special thanks to Prof. Lasantha Premarathna for sparking my interest in non-parametric statistics!

## Section 1: Proofs and Examples

Miscellaneous information such as knowledge on sets, power sets, subsets and countability from [@demirbas2023introduction].


### Example of a $\sigma$-Algebra 

For additional clarity, we provide an example of a $\sigma$-algebra $\mathcal{F}$ to demonstrate the properties mentioned in the literature review.


Let's consider the finitely countable and simple set $\mathbb{X} = \{a, b, c\}$. 


Directly, $P(\mathbb{X}) = \big\{ \emptyset, \{a\}, \{b\}, \{c\}, \{a, b\}, \{a, c\}, \{b,c\}, \{a, b, c\} \big\}$ is the power set of $\mathbb{X}$, where we note $\{a, b, c\} = \mathbb{X}$. Consider $\mathcal{F} = \big\{ \emptyset, \{a\}, \{b,c\}, \{a, b, c\} \big\} \subseteq P(\mathbb{X})$. 


We will apply the properties discussed in the literature review section to prove that $\mathcal{F}$ is a $\sigma$-algebra.


To verify Property 1 (universality and non-emptiness), we note that we can also write $\mathcal{F}$ as $\big\{\emptyset, \{a\}, \{b,c\}, \mathbb{X} \big\}$. From this definition, it is direct to see that $\mathbb{X} \in \mathcal{F}$ and $\emptyset \in \mathcal{F}$, verifying universality and non-emptiness.


To verify Property 2 (closure under complementation), we note that $A^{c^{\,c}} = A$. Hence, if we show $A \in \mathcal{F} \implies A^c \in \mathcal{F}$ this is equivalent to showing that $A \in \mathcal{F} \iff A^c \in \mathcal{F}$. Since $\mathcal{F}$ is finitely countable, we can consider a case-wise basis for verification. Firstly, we have $A = \emptyset$. By definition, $A^c = \mathbb{X}$. We see that $\mathbb{X} \in \mathcal{F}$. As mentioned before, this implies that the case where $A = \mathbb{X}$ also holds. Now, we can proceed to verify the case where $A = \{a\}$. We note that $\{a\}^c = \{b, c\}$, and that $\{b, c\} \in \mathcal{F}$. Therefore, this case holds and thus so does the case where $A = \{b, c\}$ by biconditionality. Hence, we can conclude that $\mathcal{F}$ is closed under complementation.


To verify Property 3 (closure under countable unions), we first consider the concrete case where $\{A_i\}_{i = 1}^2 = \big\{\{a\}, \{b,c\} \big\}$ where $A_1 = \{a\}$ and $A_2 = \{b, c\}$. We note that $A_1, A_2 \in \mathcal{F}$, so we would expect that $A_1 \cup A_2 \in \mathcal{F}.$ Directly, $A_1 \cup A_2 = \{a\} \cup \{b,c\} = \{a, b, c\} = \mathbb{X}$, and we see that $\mathbb{X} \in \mathcal{F}$. For the remaining cases, $\forall A \in \mathcal{F}, A \cup \emptyset = A$ by fundamental set properties, and directly $A \in \mathcal{F}$ by construction. Similarly, $\forall A \in \mathcal{F}, A \cup \mathbb{X} = \mathbb{X}$ and we know that $\mathbb{X} \in \mathcal{F}$. Hence, for all cases, $\mathcal{F}$ is closed under countable unions.


From all of these properties, we can conclude that $\mathcal{F}$ is a $\sigma$-algebra, as required $\square$.


### Example of a Probability Measure

Using the $\sigma$-algebra $\mathcal{F} = \big\{ \emptyset, \{a\}, \{b,c\}, \{a, b, c\} \big\}$ we will define $\upmu: \mathcal{F} \mapsto [0, 1]$ and prove that $\upmu$ is a probability measure using the properties discussed in the literature review. 

We will define a concrete example as follows, and show it is a probability measure on $\mathcal{F}$.
$$
\upmu(A) = \begin{cases}
2/3, & |A| = 1 \\
1/3, & |A| = 2 \\
1, &  |A| = 3 \\
0, & \text{otherwise} \end{cases}
$$

 Let's evaluate the properties discussed in the literature review to verify that $\upmu$ is in fact a probability measure.

First, we evaluate Property 1 (non-negativity.)  In effect, we wish to verify that $\forall A \in \mathcal{F}, \upmu(A) \geq 0$. We can easily evaluate the universal in a case-wise basis. 

\vspace{-1mm}
  a. $\emptyset \in \mathcal{F}$ and $\upmu(\emptyset) = 0 \geq 0$.
  b. $\{a\} \in \mathcal{F}$ and $\upmu(\{a\}) = 2/3 \geq 0$.
  c. $\{b,c\} \in \mathcal{F}$ and $\upmu(\{b,c\}) = 1/3 \geq 0$.
  d. $\mathbb{X} \in \mathcal{F}$ and $\upmu(\mathbb{X}) = 1 \geq 0$.



Hence, $\forall A \in \mathcal{F},  \upmu(A) \geq 0$, verifying the non-negativity clause. Further, we see that $\forall A \in \mathcal{F}, 0 \leq \upmu(A) \leq 1$.


In addition, we can utilize the evaluations above to verify Property 2. Directly, we see that  $\upmu(\emptyset) = 0$ and $\upmu(\mathbb{X}) = 1$, verifying Property 2  that a value of $1$ is assigned to the sample space $\mathbb{X}$.



Finally, we verify Property 3 (countable additivity.) Again, because the $\sigma$-algebra is finitely countable, we verify all pairwise disjoint intersections on a case-wise basis.


  a. First, we consider the set of pairwise disjoint sets $\{\emptyset, A\}$ for $A \in \mathcal{F}$. We see that $\forall A \in \mathcal{F}, \emptyset \cap A = \emptyset$ and $\emptyset \cup A = A$. Using, $\upmu(\emptyset) = 0$, we observe that $\upmu\Big(\bigcup_{i} A_i \Big)  = \upmu(A) =  \upmu(A) + {\upmu(\emptyset)} = \sum_{i=1} \upmu(A_i)$, as required. 
  b. Similarly, we consider $\mathbb{X} \in \mathcal{F}$, noting that $\forall A \in \mathcal{F}, \mathbb{X} \cap A = A$ is non-disjoint, so the case holds vacuously by falsity of the antecedent. A similar argument can be applied for $\{A, A\}, A \in \mathcal{F}$ which is evidently a non-disjoint pair. 
  c. The other pairwise disjoint set in $\mathcal{F}$ is $\{A_i\}=\big\{\{a\}, \{b, c\}\big\}$, since $\{a\} \cap \{b, c\} = \emptyset$. Hence, this pair should be countably additive. We can see directly that $\bigcup_{i} A_i = \{a\} \cup \{b, c\} = \mathbb{X}$, Hence, $\upmu\Big(\bigcup_{i} A_i \Big) = \upmu(\mathbb{X}) = 1$, so we anticipate $\sum_i \upmu(A_i) = 1$. To verify, we see that  $\sum_i \upmu(A_i) = \upmu(\{a\}) + \upmu(\{b, c\})   = 2/3 + 1/3 = 1$, so countable additivity holds. 


From all of these properties, we can conclude that $\upmu$ is a probability measure on $\sigma$-algebra $\mathcal{F}$, as required $\square$.

### Proof of Gamma-Poisson Conjugacy : Posterior


We noted the posterior conjugate in the methods section, and left the full derivation of the conjugate pair for the appendix here. As mentioned in the methods section, the conjugate pair is helpful for Gibbs samplers where the nonparametric Dirichlet Process is centered about a gamma base measure.

Let $\lambda \sim \text{gamma}(\alpha, \beta)$ be the prior on the Poisson rate parameter $\lambda$. Let $x_i \mid \lambda \sim \text{pois}(\lambda)$ for $i = 1, 2, \dots, n$.
<!-- -->
We derive the expression for the conjugate prior, beginning from the well-known expression for the posterior. 
$$
\begin{aligned}
\text{Posterior} &\propto \text{Prior } \times \text{ Likelihood} \\
\text{Posterior} &\propto p_{\text{gam}}(\lambda ; \alpha, \beta) \times \prod_{i = 1}^n \, p_{\text{pois}}(x_i\, ; \lambda) \\
\text{Posterior} &\propto  p_{\text{gam}}(\lambda ; \alpha, \beta) \times \prod_{i = 1}^n \, \Big( \dfrac{e^{-\lambda} \lambda^{x_i}}{x_i!} \Big) \\
\text{Posterior} &\propto  p_{\text{gam}}(\lambda ; \alpha, \beta)\times \prod_{i = 1}^n (e^{-\lambda})\cdot  \prod_{i = 1}^n (\lambda^{x_i}) \cdot  \prod ({x_i!})^{-1} \\
\text{Posterior} &\propto  \Big( \dfrac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha - 1}e^{-\beta \lambda} \Big) \times (e^{-n\lambda})\cdot  (\lambda^{\sum x_i}) \cdot  \prod_{i=1}^n ({x_i!})^{-1} \\
\text{Posterior} &\propto  \underbrace{\Big( \dfrac{\beta^{\alpha}}{\Gamma(\alpha) \prod_{i=1}^nx_i!} \Big)}_{\text{constant wrt } \lambda}   \cdot (e^{-n\lambda}e^{-\beta \lambda})\cdot  (\lambda^{\sum x_i}\lambda^{\alpha - 1})   \\
\text{Posterior} &\propto  (e^{-n\lambda -\beta \lambda}) \cdot  (\lambda^{\sum x_i +\alpha - 1})  \\
\text{Posterior} &\propto  (e^{-\lambda (n+\beta)}) \cdot  (\lambda^{ \sum x_i +\alpha - 1})   \\
\text{Posterior} &\propto   \text{gam}(\alpha + \sum_{i = 1}^n x_i, \beta + n) \\
\end{aligned}
$$
The above gives the Posterior Distribution used in Part 3 of the DPMM Mixing Distribution definition, as required. 



### Proof of Gamma-Poisson Conjugacy : Posterior Predictive


As was discussed in the main work, the posterior predictive is also needed to use the sampler. Since we need the full expression (not a proportionality), we utilize line 5 from the previous proof for a single observation.

For a single observation, we have the following from the gamma distribution using $x_n$ and $1$ rather than $n$ and the observation sum. 
$$
\begin{aligned}
p(\lambda \mid x_n) &=  \Big( \dfrac{(\beta + 1)^{\alpha +x_n}}{\Gamma(\alpha + x_n)} \lambda^{\alpha + x_n - 1}e^{-(\beta + 1) \lambda} \Big) 
\end{aligned}
$$
Then, we compute $P(x_{n+1})$ by marginalization to arrive at the predictive distribution.
$$
\begin{aligned}
p(x_{n+1} \mid x_n ) &=  \int_{0}^{\infty} p(x_{n + 1} \mid \lambda)p(\lambda \mid x_n) \text{d}\lambda \\
p(x_{n+1} \mid x_n ) &=  \int_{0}^{\infty} p(x_{n + 1} \mid \lambda)\bigg( \dfrac{(\beta + 1)^{\alpha +x_n}}{\Gamma(\alpha + x_n)} \lambda^{\alpha + x_n - 1}e^{-(\beta + 1) \lambda} \bigg) \text{d}\lambda \\
p(x_{n+1} \mid x_n ) &=  \int_{0}^{\infty} \Big( \dfrac{e^{-\lambda} \lambda^{x_{n+1}}}{x_{n+1}!} \Big) \bigg(  \dfrac{(\beta +1)^{\alpha +x_n}}{\Gamma(\alpha + x_n)} \lambda^{\alpha + x_n - 1}e^{-(\beta + 1) \lambda} \bigg) \text{d}\lambda \\
p(x_{n+1} \mid x_n ) &=  \frac{(\beta+1)^{\alpha +x_n}}{{\Gamma(\alpha + x_n)x_{n+1}!}}\int_{0}^{\infty}   e^{-\lambda} \lambda^{x_{n+1}}  \Big( \lambda^{\alpha + x_n - 1}e^{-(\beta + 1) \lambda} \Big) \text{d}\lambda \\
p(x_{n+1} \mid x_n ) &=  \frac{(\beta+1)^{\alpha +x_n}}{{\Gamma(\alpha + x_n)x_{n+1}!}}\int_{0}^{\infty}   \Big( \lambda^{\alpha + x_n + x_{n+1}- 1}e^{-(\beta + 2) \lambda} \Big) \text{d}\lambda
\end{aligned}
$$
At this point, we recall the definition of the complete gamma function.
$$
\Gamma(z) = \int_0^{\infty}t^{z-1}e^{-t}\text{d}t, \text{ where } \; \mathfrak{R}(z) > 0
$$
Note by the strictly real-valued supports and parameters of both the Gamma and Poisson distributions that the $\mathfrak{R}(z) > 0$ clause holds trivially in this case. 

For our expression, we will proceed with substitution to get it into this form.

First, we let $t = (\beta + 2)\lambda$. To solve via substitution, we note that:
$$
\text{d}t = (\beta + 2)\text{d}\lambda, \;\text{ hence } \; \text{d}\lambda = \dfrac{\text{d}t}{(\beta + 2)}
$$
Let us substitute this value into our expression and simplify.
$$
\begin{aligned}
p(x_{n+1} \mid x_n ) &=  \frac{(\beta+1)^{\alpha +x_n}}{{\Gamma(\alpha + x_n)x_{n+1}!}}\int_{0}^{\infty}   \bigg( \dfrac{t}{(\beta + 2)}\bigg)^{\alpha + x_n + x_{n+1}- 1}(e^{-t}) \dfrac{\text{d}t}{(\beta + 2)} \\
p(x_{n+1} \mid x_n ) &=  \frac{(\beta+1)^{\alpha +x_n}}{ \Gamma(\alpha + x_n) x_{n+1}!(\beta + 2)^{x_n + x_{n+1}-1}(\beta + 2) } \int_{0}^{\infty}   t^{\alpha + x_n + x_{n+1}- 1}(e^{-t}) \text{d}t \\
p(x_{n+1} \mid x_n ) &=  \frac{(\beta+1)^{\alpha +x_n}}{ \Gamma(\alpha + x_n) x_{n+1}!(\beta + 2)^{\alpha + x_n + x_{n+1}-1}(\beta + 2)^1 } \Gamma( \alpha +x_n + x_{n+1} )\\
p(x_{n+1} \mid x_n ) &=   \frac{(\beta+1)^{\alpha +x_n} \Gamma( \alpha +x_n + x_{n+1} )}{ \Gamma(\alpha + x_n) x_{n+1}!(\beta + 2)^{\alpha +x_n + x_{n+1}} }  \;\;\; \square
\end{aligned}
$$
The above gives the Posterior Predictive used in Part 4 of the DPMM Mixing Distribution definition, as required. 


## Section 2: R Code

### Results Code
```{r, eval = FALSE}
library(dirichletprocess)

# set seed for reproducibility
set.seed(447)
# start the clock
start_time = proc.time()
M = 10000
RUN = FALSE # TRUE if running sampler

###########################
### Mixing Distribution ###
###########################
# define the framework conjugate mixture model
poisMd = MixingDistribution(
  distribution = "poisson",
  priorParameters = c(1, 2),
  conjugate = "conjugate"
)

# Part 1: Poisson Likelihood
Likelihood.poisson = function(mdobj, x, theta){
  return(as.numeric(dpois(x, theta[[1]])))
}
# Part 2: Gamma Prior : Base Measure
PriorDraw.poisson = function(mdobj, n){
  draws = rgamma(n, mdobj$priorParameters[1], mdobj$priorParameters[2])
  theta = list(array(draws, dim=c(1,1,n)))
  return(theta)
}
# Part 3: Posterior Draw (defined by conjugacy)
PosteriorDraw.poisson = function(mdobj, x, n=1){
  priorParameters = mdobj$priorParameters
  theta = rgamma(n, priorParameters[1] + sum(x),
                     priorParameters[2] + nrow(x))
  return(list(array(theta, dim=c(1,1,n))))
}

# Part 4: Predictive Distribution by Marginalization
Predictive.poisson = function(mdobj, x){
  priorParameters = mdobj$priorParameters
  pred = numeric(length(x))
  for(i in seq_along(x)){
    alphaPost = priorParameters[1] + x[i]
    betaPost = priorParameters[2] + 1
    pred[i] = (priorParameters[2] ^ priorParameters[1]) / gamma(priorParameters[1])
    pred[i] = pred[i] * gamma(alphaPost) / (betaPost^alphaPost)
    pred[i] = pred[i] * (1 / prod(factorial(x[i])))
  }
  return(pred)
}


###########################
### D.P. Gibbs Sampling ###
###########################

# read in cleaned data frame
df = read.csv("final_project/cleaned_crash_data.csv")
# monthly crash count, in 100s of crashes 
y = ( round((df$crash_count)/100) )

# create DP Poisson Mixture Model from mix dist. defined earlier
dirp = DirichletProcessCreate(y, poisMd)

if(RUN){
  # initialize and fit DPMM via Gibbs 
  dirp = Initialise(dirp)
  dirp = Fit(dirp, M) 
  
  # compute, posterior frame: sampling from the posterior
  cat("Generating Posterior Frame...")
  # include 95% and 99% Credible Intervals
  postf = PosteriorFrame(dirp, 0:22, 10000, ci_size = c(0.1, 0.01))
  # save to avoid repeat simulation
  saveRDS(postf, file = "final_project/posterior_sampleframe.RDS")
  saveRDS(dirp, file =  "final_project/posterior_results.RDS")
}
# report runtime 
total_time = proc.time() - start_time
cat("Total Runtime of Script: ", total_time['elapsed'], "seconds\n")
```
### Data Processing Code
```{r, eval=FALSE}
library(dplyr)
library(lubridate)
library(readxl)
library(tidyr)

#######################
### Data Processing ###
#######################

# NOTE: the aggregation is over a very large dataset, so runtime is slow

cat("Reading Data... \n")
# read Chicago Crash data from excel using `readxl`
df <- read_excel("final_project/crash_dates_and_damages.xlsx")

cat("Processing Dates...\n")
df <- df %>%
  # drop weirdly formatted excel dates
  filter(!grepl("^\\d+\\.\\d+$", CRASH_DATE)) %>%  
  # standrdize remaining dates to same timezone
  mutate(CRASH_DATE = mdy_hms(CRASH_DATE, tz = "UTC", quiet = TRUE)) %>%  
  # remove NAs
  drop_na(CRASH_DATE) %>% 
  # standardize formatting to MDY
  mutate(CRASH_DATE = format(CRASH_DATE, "%m/%d/%Y"))  

cat("Aggregating... \n")
# here, we aggregate severity counts weekly  
aggregated_data <- lapply(damage_levels, function(damage_level) {
  # we filter through each damage level
  df_filtered <- df %>% filter(DAMAGE == damage_level)
  # and aggregate by week 
  weekly_aggregation <- df_filtered %>%
    mutate(Week = floor_date(as.Date(CRASH_DATE, format = "%m/%d/%Y"), "week")) %>%
    group_by(Week) %>%
    summarise(Crash_Count = n(), .groups = 'drop') %>%
    arrange(Week)
  # use NNI for poorly-captured 2014, 2015 data
  cat("Imputing ", damage_level, "...\n")
  # get 2016 weeks
  weeks_2016 <- weekly_aggregation %>%
    filter(year(Week) == 2016) %>%
    pull(Week)
  # get exact week if present, else nearest week by euclidean distance
  weekly_aggregation <- weekly_aggregation %>%
    rowwise() %>%
    mutate(
      Nearest_2016_Week = if(year(Week) %in% c(2014, 2015)) {
        weeks_2016[which.min(abs(difftime(Week, weeks_2016, units = "weeks")))]
      } else {
        Week
      }
    ) %>%
    left_join(weekly_aggregation %>% filter(year(Week) == 2016) %>%
                select(Week, Crash_Count), by = c("Nearest_2016_Week" = "Week")) %>%
    mutate(
      # Use 2016's Crash_Count for nearest week and add to scaled 2014 count if present
      Crash_Count_Adjusted = if_else(year(Week) == 2014, Crash_Count.y + Crash_Count.x / max(Crash_Count.x) * 2, Crash_Count.x)
    ) %>%
    select(Week, Crash_Count_Adjusted)
  
  return(weekly_aggregation)
})

# the data frame we use is then the aggregated weeks & counts
outDF = data.frame(
  crash_time = c(
    aggregated_data[[1]]$Week,
    aggregated_data[[2]]$Week,
    aggregated_data[[3]]$Week
  ),
  crash_count =  c(
    aggregated_data[[1]]$Crash_Count_Adjusted,
    aggregated_data[[2]]$Crash_Count_Adjusted,
    aggregated_data[[3]]$Crash_Count_Adjusted
  )
)

# which we write to a csv
cat("Writing to File... \n")
write.csv(outDF, "final_project/cleaned_crash_data.csv", row.names = FALSE)
```
<!-- $$ -->
<!-- \mathbb{G}_0 : \big\langle \text{dir}({\alpha_{j,1:K(j)}}), N(m_{j,k}, s^2_{j,k}) \big\rangle =  \big\langle \text{dir}({1_{1:4}}), N(0, 10) \big\rangle -->
<!-- $$ -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \mathcal{P} &\sim \text{DP}(\alpha \mathbb{G}_0) \\ -->
<!-- \uptheta_i \mid \mathcal{P} &\sim \mathcal{P} \\ -->
<!-- X_{i,j} \mid \uptheta_{i, x} &\sim f_x(x \mid  \uptheta_{i, x})   \\ -->
<!-- X_{i,j} \mid \uptheta_{i, y} &\sim f_y(y \mid  \uptheta_{i, x})   -->
<!-- \end{aligned} -->
<!-- $$ -->

$$
\begin{aligned}
\{\pi_k\}_{k=1}^K &\sim \text{GEM}(\alpha),  \text{ stick-breaking process} \\
\{\uptheta_k\}_{k=1}^K &\sim \mathbb{G}_0, \text{ base measure}\\
z_i&\sim \text{categorical}(\{1, 2, \dots, K\}, \{\pi_k\}), \text{ cluster assignment }  i \in [1, N]\\
y_i \mid z_i, \{\uptheta_k\}_1^K  &\sim \text{F}(x_i \,;\,\uptheta_{z_i}),  \text{ likelihood given cluster}\\
\text{Infinite Dirichlet} &\text{ Process Mixture Model} \\
\{\pi_k\}_{k=1}^{\infty} &\sim \text{GEM}(\alpha)\\
\{\langle \alpha_k, \beta_k \rangle\}_{k=1}^{\infty} &\sim \mathbb{G}_0 \\
y_i \mid \{\pi_k\}, \{\langle \alpha_k, \beta_k \rangle\} &\sim \sum_{k = 1}^{\infty} \pi_k \text{beta}(y_i \mid \langle \alpha_k, \beta_k \rangle) \\
\end{aligned}
$$

Where the MCMC implementation is facilitated by the `dirichletprocess` package, created by [@dirichletprocess2023].

Finite-approximated infinite mixture DPMM, similar 
$$
\begin{aligned}
\{\pi_k\}_{k=1}^K &\sim \text{GEM}(\alpha),  \text{ stick-breaking process} \\
\{\uptheta_k\}_{k=1}^K &\sim \mathbb{G}_0, \text{ base measure}\\
z_i&\sim \text{categorical}(\{1, 2, \dots, K\}, \{\pi_k\}), \text{ cluster assignment }  i \in [1, N]\\
y_i \mid z_i, \{\uptheta_k\}_1^K  &\sim \text{F}(x_i \,;\,\uptheta_{z_i}),  \text{ likelihood given cluster}
\end{aligned}
$$
<!-- So, we would... -->
<!-- Define prior $\mathcal{P} \sim \text{DP}(\alpha \mathbb{G}_0)$ -->
<!-- The implemented code for the Gibbs Sampler is modified from the DPMM code by [@dpmm2023]  -->
$$
\begin{aligned}
\{\pi_k\}_{k=1}^{\infty} &\sim \text{GEM}(\alpha),  \text{ stick-breaking process} \\
\{\uptheta_k\}_{k=1}^{\infty} &\sim \mathbb{G}_0, \text{ base measure},\\
F &= \prod_{i=1}^N\bigg(\sum_{k =1}^{\infty} \pi_k N(x_i \mid \uptheta_k) \bigg), \text{ likelihood from normal kernel}
\end{aligned}
$$



## Sources
[@jonesDPMM]