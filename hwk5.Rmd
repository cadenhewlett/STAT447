---
title: "STAT 447 Assignment 5"
author: "Caden Hewlett"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{bbm}
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(extraDistr)
library(ggplot2)
library(pracma)
library(distr)
library(latex2exp)
source("scaffold.R")
source("simple.R")
source("simple_utils.R")
```

# Question 1: Sequential Updating

Consider a joint a joint probabilistic model given by
$$
\theta \sim \rho, \text{ and } (x_i \mid \theta) \; \overset{iid}{\sim} \nu_{\theta}, \text{ where } i \in \{1, 2, \dots , n\}
$$
 
where $\rho$ is a prior distribution for the unknown parameter $\theta$, and $\{x_i\}_{i=1}^n$ is a sequence of observations with conditional distribution $\nu_{\theta}$.

## Part 1

Write down the posterior distribution of $\theta$ given $\{x_i\}_{i=1}^n$.

In a more verbose sense, let $\Theta$ be the random variable for the unknown parameter, and $\theta$ be the realization. We can then write. 

$$
\text{ Prior follows } p_{\Theta}(\theta), \text{ where } p_{\Theta}(\theta) \text{ is the PMF/PDF given by } \rho
$$
$$
P(X = x \mid \theta ) = p_{X \mid \theta}(x, \theta), \text{ where } p_{X \mid \theta}(x, \theta) \text{ is the PMF given by } \nu_{\theta}
$$
With this in mind, we can write the posterior of $\theta$ given $\{x_i\}_{i=1}^n$, where we describe the event that $\{X_i\}_{i=1}^n = \{x_i\}_{i=1}^n$ using Bayes' Rule.

We will begin by using the most verbose notation possible, for complete clarity.
$$
P(\Theta =\theta \mid \{X_i\}_{i=1}^n = \{x_i\}_{i=1}^n) = \dfrac{P(\Theta = \theta)P\big(\{X_i\}_{i=1}^n = \{x_i\}_{i=1}^n \mid \Theta =\theta\big)}{P\big(\{X_i\}_{i=1}^n  = \{x_i\}_{i=1}^n\big)}
$$
We start by considering the joint likelihood function.
$$
P\big(\{X_i\}_{i=1}^n = \{x_i\}_{i=1}^n \mid \Theta =\theta\big) = P\big((X_n = x_n, X_{n-1} = x_{n - 1}, \dots, X_1 = x_1) \mid \Theta = \theta \,)
$$
Then, using intersections, we can write the likelihood as:
$$
P\big(\{X_i\}_{i=1}^n = \{x_i\}_{i=1}^n \mid \Theta =\theta\big) = P\Big(  \bigcap_{i = 1}^n  (X_i = x_i) \mid \Theta = \theta \Big)
$$
Now, we use the following property of *iid* random variables
$$
\text{I.I.D} \implies \forall (i \neq j) \in [1,n], \; X_i \perp X_j \implies \forall (i \neq j) \in [1,n], P(X_i \cap X_j) = P(X_i)P(X_j)
$$
Hence, we can write the likelihood as:
$$
P\big(\{X_i\}_{i=1}^n = \{x_i\}_{i=1}^n \mid \Theta =\theta\big) = \prod_{i = 1}^n \Big( P(X_i = x_i \mid \Theta = \theta ) \Big) = \prod_{i = 1}^n(\nu_{\theta}) = (\nu_\theta)^n
$$
Which, as you can see, simplifies nicely to $(\nu_{\theta})^n$.

<!-- (x_i \mid \theta) \; \overset{iid}{\sim} \nu_{\theta}, \text{ where } i \in \{1, 2, \dots , n\} -->

## Part 2

Suppose now we get an additional data point $x_{n + 1}$ with the same conditional distribution $\nu_{\theta}$. Show that using the posterior from part 1 as the *prior* and data equal to just $x_{n + 1}$ gives the same posterior distribution as redoing part 1 with the $n + 1$ data points.

# Question 2: Baesian Inference in the Limit of Increasing Data